\section{Lessons Learned}
\label{sec:lesson-learned}
Here we present the lessons we learned during our semantics development,
     identifying important aspects to be considered, and clarifying best
     practices for developing a large ISA semantics.  We also discuss the
     novel aspects of our semantics development approach that allow us to
     obtain a complete and faithful semantics with a practical amount of
     effort.

\paragraph{Automatic semantics synthesis}

Most previous efforts in formalizing \ISA semantics can be categorized based on whether the underlying approach is fully manual ~\cite{Goel:FMCAD14, TSL:TOPLAS13, Leroy:2009, sail-x86} or fully automatic~\cite{Heule2016a, Roessle:CPP19, Hasabnis:ASPLOS16, Hasabnis:FSE16}. We note that none of these approaches, when used in isolation, sufficiently scale to a complete and faithful semantics, as much as ours that combines these complementary approaches so that they benefit from each other.

Section~\ref{sec:Approach:Overview} reports the challenges we encountered in achieving fully automatic synthesis of the entire \ISA semantics.
Specifically, in a vast instruction set like \ISA \cmt{computer (CISC) architecture such as x86}, it is common that many instructions can be grouped together where the instructions of each group are similar to each other except for a few differences.
%One approach to automatic semantics synthesize is to employ stratification approach~\cite{Heule2016a} to learn the semantics of instruction variants in a group using the representatives of the group.
An automatic synthesis technique leveraging such a group, such as the stratification approach~\cite{Heule2016a}, would effectively synthesize such instruction variants' semantics, provided that the semantics of representative instructions in each group are given in advance.\footnote{For certain complex instructions, the size of their group is very small (i.e., they are quite different to each other), and thus the automatic synthesis would not yield a sufficient gain over the effort of specifying the semantics of their representatives, but we found that the number of such isolated instructions of x86-64 is small.}
The problem, however, is that it is non-trivial to properly partition all the instructions into such groups, providing the representative instruction semantics for each group, \emph{without} a priori knowledge about the semantics of all instructions.
%The stratification approach~\cite{Heule2016a} had been proposed to solve this dilemma, but it turned out to be not sufficient, leaving a substantial part of semantics unspecified.
The vanilla stratification approach~\cite{Heule2016a} turned out to be not sufficient to solve this dilemma, leaving a substantial part of the semantics unspecified.
Thus, we decided to manually provide the information about the partition and representatives, for which we had to consult the manual to obtain knowledge about the remaining part of semantics.
Once we obtained the required knowledge, however, we realized that it would be more straightforward to directly turn the knowledge into the semantics than going through the synthesis process, and thus we ended up manually specifying the remaining part of the semantics.

%As mentioned in Section~\ref{sec:Approach:Overview}, we found that our ideas of extending \Strata do not scale because we need information about the instructions, whose semantics we want to learn, in order to constraint the search space. This lesson suggested a new idea: the information needed to reduce the search space can be automatically extracted from the manual. 
%This  information does not need to be precise, and we believe that such rough information can be automatically extracted from the manual using text processing.  We plan to explore this idea while defining the semantics of unspecified and/or new instructions.

%As an alternative, we suggest a hybrid synthesis approach that we believe is practically promising, in particular for complex instruction set computer (CISC) architectures such as x86.
%
%While it is labor-intensive and error-prone to manually specify all the instruction variants, the automatic synthesis technique is shown to be effective to synthesize such instruction variants' semantics, provided that the semantics of its representative instruction is given in advance.
%
%Thus, in a hybrid approach, the semantics of representative instructions are manually written, and their variants are automatically synthesized. We believe that this hybrid approach is in a sweet spot, where the machine's search power and the human reasoning are effectively combined.
%We plan to apply and evaluate this idea when we specify the semantics of new instruction that will be introduced in the next version of x86-64 ISA.

Another important step of the semantics synthesis is post-processing. The generated semantics is often verbose and not necessarily human-readable. The post-processing step is desired to simplify the generated semantics to be succinct, which helps to increase the human-readability as well as to improve the efficiency when being employed in other applications (e.g., the size of SMT formula encoding can be reduced, which can reduce the burden of SMT solvers). For our semantics development, we have written dozens of simplification rules that are fed to the \K framework to simplify the synthesized semantics further (Section~\ref{sec:Approach}). 

\paragraph{Modeling and executing implementation-dependent behaviors}

The \ISA ISA standard admits \emph{implementation-dependent behaviors} for
certain operations on certain input patterns, that is, each processor
implementation can freely choose the execution behavior for each such case
(Section~\ref{sec:challenges-in-formalizing-x86}).
%
Faithfully modeling the implementation-dependent behaviors is necessary for the correctness of the
semantics.  For example, as mentioned in Section~\ref{subsec:compare-stoke},
  Stoke~\cite{Stoke2013} does not faithfully model such behaviors, causing
  certain errors in their semantics that we revealed~\cite{BugStoke986}.

There are two natural, faithful ways of specifying implementation dependent
behaviors.  One is to parameterize the semantics over the
implementation-dependent behaviors, and later instantiate it with a profile
that describes specific behaviors taken by the processor of interest. This
approach is desirable for validating the semantics using concrete execution.
Another is to introduce non-determinism in the semantics, which captures a set
of different possible behaviors in a single semantics, which is desirable
during symbolic interpretation of the ISA code.  We note that most of other
existing \emph{direct} \ISA semantics employ approaches similar to the ones
described above, faithfully modeling the implementation-dependent behaviors.
For example, Goel \etal~\cite{Goel:ProCoS17} models such behaviors using a
constraint function which is guaranteed to be unique and non-deterministic,
           while they employ the aforementioned profile-based approach for
           concrete execution.  TSL~\cite{TSL:TOPLAS13} makes both approaches
           available, from which their users can choose.

In our semantics, we faithfully modeled the undefined value as a unique symbol
(called undef) whose value is non- deterministically decided each time within
the proper range. For validating the semantics, we concretely executed the
semantics while the non-deterministic behaviors are represented symbolically
using the undef symbol and then we checked if the hardware output is matched by
(an instance of) the simulated output.

%The \ISA ISA standard admits \emph{implementation-dependent behaviors} for certain operations on certain input patterns, that is, each processor implementation can freely choose the execution behavior for each such case (Section~\ref{sec:challenges-in-formalizing-x86}).
%There are two natural, faithful ways of specifying implementation-dependent behaviors.
%One is to parameterize the semantics over the implementation-dependent behaviors, and later instantiate it with a profile that describes specific behaviors taken by the processor of interest.
%Another is to introduce non-determinism in the semantics, which captures a set of different possible behaviors in a single semantics.
%We took the second approach in this work since the implementation-dependent behaviors of x86 are quite limited and mostly localized (e.g., as in the example in Figure~\ref{fig:andn-semantics}).
%
%However, the non-determinism makes it non-trivial to execute (and validate) the
%semantics.  For example, during hardware co-simulation, the output of hardware
%execution may vary depending on the underlying processors, while the
%non-deterministic semantics execution will randomly choose certain behavior,
%  which may be different from the specific behavior implemented in a processor.
%  To mitigate this issue, for the hardware co-simulation, we symbolically
%  executed the semantics where the non-deterministic behaviors are represented
%  symbolically, so that the symbolic output captures all possible behaviors,
%  and then we checked if the hardware output is matched by (an instance of) the
%  symbolic output.
%
%Faithfully modeling the implementation-dependent behaviors is necessary for the correctness of the semantics.
%For example, as mentioned in Section~\ref{subsec:compare-stoke}, Stoke~\cite{Stoke2013} does not faithfully model such behaviors, causing certain errors in their semantics that we revealed~\cite{BugStoke986}.
%On the other hand, most of other existing \emph{direct} \ISA semantics employ an approach similar to the ones described above, faithfully modeling the implementation-dependent behaviors.
%For example, Goel \etal~\cite{Goel:ProCoS17} models such behaviors using a constraint function which is guaranteed to be unique and non-deterministic, while they employ the aforementioned profile-based approach for concrete execution.
%TSL~\cite{TSL:TOPLAS13} makes both approaches available, from which their users can choose.
%
%%\Comment{Instead of dropping the paragraph about the extra difficulty of using the non-determinism strategy, you need to discuss the tradeoffs. So describe both the benefits and the drawbacks of the non-determinism strategy. I'm just uncommenting the old text here, for now.}
%%%
%%Note that the non-determinism strategy makes it non-trivial to execute (and validate) the semantics.
%%For example, in the hardware co-simulation, the output of hardware execution may vary depending on the underlying processors, while the non-deterministic semantics execution will randomly choose certain behavior, which may be different from the specific behavior implemented in a processor.
%%To mitigate this issue, for the hardware co-simulation, we symbolically executed the semantics where the non-deterministic behaviors are represented symbolically, so that the symbolic output captures all possible behaviors, and then we checked if the hardware output is matched by (an instance of) the symbolic output.

\paragraph{Employing multiple semantic engineering frameworks}

We found that employing multiple semantic frameworks is helpful. Specifically, we employed the two semantic frameworks, \K and Stoke, where we enjoyed all of their (executive) benefits that make it easier for us to write and validate the semantics, and utilize the semantics in various applications. For example, we wrote the semantics of certain complicated instructions (e.g., \instr{pcmpestri}, \instr{pcmpestrm}, and \instr{pclmulqdq}) in \K, as \K provides an easy way to specify behaviors with multiple cases, while Stoke would have required us to write a big nested if-then-else expression, which is not convenient. As another example of the benefits, we used Stoke to validate most of our instruction semantics
as Stoke provides an infrastructure\footnote{Indeed, we contributed to their infrastructure as well~\cite{completing-stock,improving-stoke}.} for hardware co-simulation, whereas we employed \K to validate the semantics of floating-point instructions as Stoke does not support executing floating-point operations while \K does.

In order to use the two frameworks interchangeably, we developed a translator between the semantics of the two frameworks. To check the correctness of the translation, we verified equivalence between the original and the translated semantics for each instruction using the Z3 SMT solver.

To summarize, employing multiple frameworks with validated translation between them improved both the ease of specification (using \K) and ease of validation (using Strata), which expedited our semantics development process and thus significantly contributed to the completeness of our semantics. Moreover, we immediately benefit from all of their formal analysis tools, increasing the applicability of the semantics in various formal reasoning tasks. Existing semantics development efforts (e.g., \cite{Goel:FMCAD14,Heule2016a}), however, %(~\cite{Goel:FMCAD14,TSL:TOPLAS13,Leroy:2009,sail-x86,Heule2016a}) 
%use a single framework and do not obtain these important benefits.
employ a single framework without utilizing the potential of other frameworks, which otherwise might have improved completeness and/or faithfulness of their semantics with the same amount of effort.
%
%\cmt{
%We note that employing \Strata enables us to explore the near-completeness of its formalism, whereas \K framework provides many out-of-the-box formal analysis tools. The fact that we can use both the frameworks  interchangeably paid us off towards completeness and generality (in terms of using the semantics for formal analyses) of the formalism. On the other hand, other approaches (~\cite{Goel:FMCAD14,TSL:TOPLAS13,Leroy:2009,sail-x86})  did not explore this mutual benefit to its full potential\footnote{That, for some of these projects,  could be attributed to the fact getting a complete formal semantics is not the primary goal.}.
