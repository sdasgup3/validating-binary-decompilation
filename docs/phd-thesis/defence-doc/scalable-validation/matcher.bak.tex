\subsection{Matcher}\label{sec:matcher}
% How does the normalized outs look like
% Choice os SSA graph -> Why not conflow graph
% Why the matcher + Semantics Preserving Transformation is sufficient.
Algorithm~\ref{alg:NM} summaries our overall strategy to check equivalence 
between the IRs generated by \mcsema (\T) and \compd (\TP). Due to the nature 
of the composition, \T \& \TP are structurally very similar. 
We leverage this observation to establish 
semantic equivalence between the two using an iterative matching and pruning 
algorithm assisted by 
normalization.  The algorithm is realized by a tool 
we develop called the \matcher. Failure of the  
\matcher should be interpreted as a \cmt{potential} bug in the lifted program. 
%comparing (using \emph{Matcher}) 
%the \emph{canonical 
%representations} of the input programs
%generated using LLVM  \emph{O3} passes \& iterative pruning. 
%uisng an iterative strategy of matching the LLVM 03 assisted canonicalized 
%program and sbsequent pruning.
\newcommand\mycommfont[1]{\footnotesize\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\begin{algorithm}
    \SetKwInOut{Input}{Inputs}
    \SetKwInOut{Output}{Output}
    \Input{
        \textbf{T:} \dlifted IR. \\
        \textbf{T$^\prime$:} \compd lifted IR.
    }
    \Output{\textbf{True} $\implies$   \textbf{T} \& \textbf{T$^\prime$} 
        semantically equivalent \\
        \textbf{False} $\implies$   \textbf{T} \& \textbf{T$^\prime$} 
        non-equivalent
    }
    
    
    \BlankLine
    
    \ForEach{\textup{corresponding function pair (\F,\FP) in 
            (\T, \TP)}}{%
        
        \If{\textup{!Matcher(\F, \FP)}}{%
            \tcp{\textup{A  bug in McSema while lifting \s{F}}}
            \KwRet{false}  \\ 
        }
        
    }
    \KwRet{true}
    %\NoCaptionOfAlgo
    \caption{\textbf{Matcher Strategy}}\label{alg:NM}
\end{algorithm}


The matcher algorithm is based on the following key observations on input IR 
programs, \s{T} \& \s{T$^\prime$}: (I) Both exhibit same control-flow 
structures, (II) In \T, the addresses computations, using 
\emph{getelelementptr} (gep in short) instructions, of all the simulated 
registers 
and flags  
are hoisted in the \emph{entry} basic block. \mcsema does this as an 
optimization so that  the subsequent data-dependent  
instructions does not have to compute them again. 
Whereas, in \TP  addresses of relevant registers and/or flags are recomputed at 
every instruction site
, and 
(III) There is no alloca instruction in either \T or \TP. All the load 
(store) 
instructions are reading from 
(writing to) the Mcsema \Mcstate fields. This makes sense because for each 
input \ISA instruction, both \T \& \TP simulates its read or write behavior on 
register/flags/memory which are all modeled as fields in the \Mcstate 
structure.


%Two instructions (one from each input program) which should match, as per its 
%data-flow behavior, may not occur in  their respective matching basic blocks. 
%This 
%is because  As the two input program 
%are not exactly equal to begin with, the instruction can get reordered 
%differently,

Lets name the normalized versions of function pair (\F \& \FP) as (\FN \& 
\FNP).
The Matcher algorithm works on SSA def-use graphs\cmt{data dependence graphs}, 
\GN \& \GNP,  generated  
from \FN \& \FNP. A vertex of the graph represents an 
LLVM instruction and an edge between two vertices captures SSA def-use 
relation. \cmt{ or   
memory 
dependence relation. Memory dependence edges, extracted from alias analysis 
results, appear between LLVM load and 
stores instructions. }
\cmt{There is a particular reason why we do not add 
control-flow edges: It is evident from \emph{\textbf{Ob II}} that the input 
programs \T \& \TP, being not exactly equal to begin with, can be normalized 
differently and there is no guarantee that two matching instructions will end 
up in matching basic blocks. }
%\cmt{We 
%call an 
%instruction in \s{N} \emph{matching exactly} with 
%an instruction in \s{N$^\prime$}  if the containing sub-graphs are 
%isomorphic.} 

The graphs \GN \& \GNP fall under the following two categories. 
\paragraph{\textup{\GN} \& \textup{\GNP} are isomorphic} 
This is a simpler case where we use a subgraph-isomorphism algorithm from Saltz 
et al.~\cite{Saltz2014}, named 
\emph{dual-simulation},  to check if both \GN \& \GNP are 
subgraph-isomorphic to each other. The algorithm, in general, first 
retrieves initial potential match sets, $\Phi$,  for each vertex in one 
graph based on semantic and/or neighborhood information in the other graph.
 In our case, the initial potential match set for an instruction 
\IN in \GN contains all the instructions in \GNP which have the same 
instruction opcode. Also, if \IN has constant operands then its potential 
matches must share 
those.  
Then the algorithm use an iterative algorithm to prune out elements from the 
potential match 
set of each vertex based on its parents/child relations till it reaches a 
fixed-point.

%\cmt{Then we augmented the algorithm to infer the basic-block 
%correspondence 
%on the fly and use that information to prune out the potential sets of store 
%instructions.}

\paragraph{\textup{\GN} \& \textup{\GNP} are non-isomorphic} 
This could happen because the input functions (\F \& \FP), being not exactly 
equal to begin with (from Obs. II), undergo different 
optimizations. 

As a simple example, consider the code snippets of the normalized function pair 
(\FN \& \FNP). \FN is generated by normalizing \F, wherein the  
simulated register \& sub-register address computations  are hoisted in 
\reg{entry} block and 
re-used at use-site. Whereas, \FNP is generated from \FP where the 
simulated address is re-computed 
at every use-site. \F and \FP upon normalization undergo 
different optimizations; One with better CSE (common subexpr. elim.) than 
the other.   
\begin{lstlisting}[style=LLVMWOBORDER]
          (*\FN*)                            (*\FNP*)
%entry:                           %entry:
  %expr = gep ...                 %expr = gep 
  %cl =  gep %expr ...            %cl =  gep %expr 
  %rcx = gep %expr ...            %rcx =  gep ...   
  ...                               ... 
%somebb:                          %somebb:
  store to %rcx                      store to %rcx
  ...                                ...
%otherbb:                         %otherbb:
  store to %cl                       store to %cl  
  ...                                ... 
\end{lstlisting}
Clearly, the  \FN \& \FNP are equivalent, yet the naive isomorphism based 
matcher has to declare them as non-equivalent because the corresponding graphs 
are
not isomorphic with the node \reg{cmn\_expr} in \GN has two out-edges versus 1 
edge in \GNP. However, the key insight is: the  subgraph with nodes 
\reg{expr}, 
\reg{cl} and \s{store \%cl} in \GNP shares no data-dependent edges with the 
rest of the graph and is isomorphic with the corresponding subgraph in \GN. We 
can prune both the subgraph from their respective parent graphs and the 
residual graphs, upon re-normalization, have better opportunity to get 
normalized to isomorphic graphs.

Again, consider the code snippets below. 
%As before, \F has the   
%simulated address computation, hoisted  in \reg{entry}, re-used in all its 
%use-site.  
%Whereas, \FNP is generated from \FP where the simulated address is re-computed 
%at every use-site. 
\FP upon normalization undergo 
partial-redundancy-elimination to make the computation of \s{rax} 
\emph{available} in both the paths (\reg{b0} and \reg{b1}), but missed 
 the opportunity to eliminate the common-subexpression, despite of the fact 
 that \s{pre\_rax} has no data-dependence on ``some-code''.      

\begin{lstlisting}[style=LLVMWOBORDER]
         (*\FN*)                            (*\FNP*)
%entry:                         %entry:
 %rax =  gep ...                 ...   
 ...
 br %some_cond, %b0, %b1         br %some_cond, %b0, %b1
%b0:                           %b0:
    .. some-code ..               .. some-code ..
                                 pre_rax = gep ...
    br %merge                    br %merge
%b1:                           %b1: 
    store to %rax                rax1 = gep ... 
                                 store to %rax1   
    ; ...                        ; ...
    br %merge                    br %merge
%merge:                        %merge: 
    store to %rax                rax2 = (*$\phi$*) [rax1, %b1], 
                                         [%pre_rax, %b0 ]
                                 store to %rax2
\end{lstlisting}
As before, despite the equivalence of \FN \& \FNP, the naive 
matcher will fail to proof graph-isomorphism, resulting in false alarm.
However, if we find the sub-graphs corresponding to ``some-code'' on either 
side are matching exactly, then we can follow  the pruning strategy as before, 
followed by normalization, and converge to isomorphic graphs.

With that insight, we design the following iterative matching and pruning 
algorithm (Algorithm~\ref{alg:Match}).  
\begin{algorithm}
    \SetKwInOut{Input}{Inputs}
    \SetKwInOut{Output}{Output}
    \Input{ \\
        \textbf{\F:} \dlifted function. \\
        \textbf{\FP:} \compd lifted function.
    }
    \Output{Check if \textbf{\F} \& \textbf{\FP} are semantically 
        equivalent}
    \BlankLine
        itr $\gets$ MaxIter \\
        \While{\textup{itr != 0}} {
            $\FN \gets \textup{llvm-opt -O3 } (\F)$ \\
            $\FNP \gets \textup{llvm-opt -O3 } (\FP)$ \\
            
            \GN $\gets$  SSA def-use graph of \FN \\
            \GNP $\gets$ SSA def-use graph of \FNP \\
            
            \If{\GN \& \GNP are isoporphic} {
                \KwRet{true}
            }
            
            $(\F, \FP) \gets \textup{Prune isoporphic subgraphs from \GN \& 
            \GNP}$
            \BlankLine
            itr $\gets$ itr - 1 \\
        }
        \KwRet{false}         
    %\NoCaptionOfAlgo
    \caption{\textbf{Matcher}}\label{alg:Match}
\end{algorithm}
The Matcher algorithm, being agnostic of the normalization pass, tries to 
recover missed optimization opportunities during normalization. This ensures 
that there will be very less false alarms.

However, as we are not considering the memory dependence edges into account, 
the Matcher might miss some real bugs. For example, consider the case where in 
\FP, there is a real memory dependence between a store and load and due to some 
bug in \mcsema, the dependence is skipped in \F. 

We consciously decided not to add memory dependence edges in the graphs because 
adding many such 
conservative edges prevents the Matcher from pruning subgraphs. Some of those 
edges could have been deleted using \emph{noAlias} information, 
but typically there exist very few \emph{noAlias} relation in a lifted program 
because of the pervasive integer to pointer conversions.



%\todo[inline]{The soundness problem that Vikram pointed out: Removing code
%    might introduce undefined behavior, which the later opt passes might abuse
%    and we might end up getting false positive or false negatives.}
%\begin{algorithm}
%    \SetKwInOut{Input}{Inputs}
%    \SetKwInOut{Output}{Output}
%    \Input{ \\
%        \textbf{\GN:} SSA graph of \N. \\
%        \textbf{\GNP:} SSA graph of  \NP.
%    }
%    \Output{Check if \textbf{T} \& \textbf{T$^\prime$} are semantically 
%        equivalent}
%    \BlankLine
%
%    changed $\gets$ true \\
%    \While{changed} { 
%       changed $\gets$ false \\
%       \For{\un $\gets$ \GN } {
%           \For{ \up $\gets$ \GN.adj(\un)} {
%               \potpup $\gets$ $\emptyset$ \\
%               \For{ \vn $\gets$ \potu } {
%                   \potvup $\gets$ \GN.adj(\vn) $\cap$ \potup \\
%                   \If{\potvup = $\empty$} {
%                       remove \vn from \potu \\
%                       \If{\potu = $\emptyset$} {
%                           \KwRet{$\emptyset$} \\
%                       }
%                       changed $\gets$ true \\
%                   }
%                   \potpup $\gets$ \potpup $\cap$ \potvup \\
%               }
%               \If{ \potpup = $\emptyset$} {
%                   \KwRet{$\emptyset$} \\
%               }
%               \If{ \potpup is smaller than \potup} {
%                   changed $\gets$ true \\
%               }
%               \potup $\gets$ \potup $\cap$ \potpup \\
%           }
%       }
%   }
%   \KwRet{$\emptyset$} \\
%    %\NoCaptionOfAlgo
%    \caption{\textbf{Dual Simulation}}\label{alg:DS}
%\end{algorithm}

 

\paragraph{Comparision with LLVM-MD \& Peggy}
At this point, it is important to differentiate our approach to establish 
equivalence between two \LLVM programs, using  normalization followed by 
matching, 
from some of the existing
approaches for validating LLVM IR-to-IR optimization
passes, e.g. LLVM-MD~\cite{Tristan:2011} and Peggy~\cite{Stepp:2011}, which, 
like our approach, move away of simulation proofs, and instead use graph 
isomorphism techniques to prove equivalence. 
Both build graphs of expressions for each program, 
transform the graphs via a series of ``expert-provided'' rewrite rules, and 
check for equality. The rewrite-rules mimic various compiler-IR optimizations 
and hence the technique is precise when the output program is an 
optimization of input program and the optmizations are captured by the rewrite 
rules. 

In our case, \s{T} (\dlifted program) and \s{T$^\prime$} (\compd lifted 
program) are structurally separated by idioms which 
might be beyond the reasoning of conservation  compiler opt. passes to 
converge them to syntactic equality. 
Encoding such idioms as rewrite rules would make the 
approach tied to a specific lifter, which is something we avoided by 
making the matcher iterative. \todo[inline]{too heavy-weight for our purpose}

%\paragraph{Semantics Preserving Transformation}




%Matching expressions with complex Ï†-nodes seems well within
%the reach of any SMTprover. Our preliminary experiments with Z3 suggest that
%it can easily handle the sort of equivalences we need to show. However, this
%seems like a very heavy-weight tool. One question in our minds is whether or
%not there is an effective tech- nique somewhere in the middle: more
%sophisticated than syntactic matching, but short of a full SMT prover.
%\begin{lstlisting}[style=LLVMWOBORDER]
%          (*\F*)                            (*\FN*)
%%entry:                                 %entry:
% ; chain of geps to step                  %comon_expr = gep ...    
% ; thr. nested State struct               %cl =  gep %common_expr  
% ; to compute address of cl               %rcx = gep %common_expr
% %cl =  gep ...                           ...
%
% ; chain of geps to step
% ; thr. nested State struct
% ; to compute address of rcx 
% %rcx =  gep ...  
% ...
%%somebb:                                %somebb:
% store to %rcx                            store to %rcx                        
% ...                                      ...  
%%otherbb:                               %otherbb:   
% store to %cl                             store to %cl
% ...                                      ... 
%\end{lstlisting}
%Next,  looks at the code snippets of \FP and its normalized version \FNP.
%\begin{lstlisting}[style=LLVMWOBORDER]
%          (*\FP*)                            (*\FNP*)
%%entry:                                 %entry:
%                                          %common_expr = gep ...
%                                          %cl =  gep ... 
%                                          %rcx =  gep ... 
% ...                                      ...
%%somebb:                                %somebb:
% ; chain of geps            
% %cl =  gep ...
% store to %rcx                            store to %rcx                        
% ...                                      ...  
%%otherbb:                               %otherbb:   
% ; chain of geps
% %rcx =  gep ...  
% store to %cl                             store to %cl
% ...                                      ... 
%\end{lstlisting}
