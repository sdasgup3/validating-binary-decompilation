\subsection{Matcher}\label{sec:matcher}
% How does the normalized outs look like
% Choice os SSA graph -> Why not conflow graph
% Why the matcher + Semantics Preserving Transformation is sufficient.
Algorithm~\ref{alg:NM} summarizes our overall strategy to check equivalence 
between the IRs generated by \mcsema (\T) and \compd (\TP). Due to the nature 
of the composition, \T \& \TP are structurally very similar. 
We leverage this observation to establish 
semantic equivalence between the two using a graph-isomorphism based 
algorithm assisted by 
normalization.  The algorithm is realized by a tool 
we develop called the \matcher. If the   
\matcher fails to match \T \& \TP, there is a \emph{potential} bug in the lifted program. 
%comparing (using \emph{Matcher}) 
%the \emph{canonical 
%representations} of the input programs
%generated using LLVM  \emph{O3} passes \& iterative pruning. 
%uisng an iterative strategy of matching the LLVM 03 assisted canonicalized 
%program and sbsequent pruning.
\newcommand\mycommfont[1]{\footnotesize\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\begin{algorithm}
    \SetKwInOut{Input}{Inputs}
    \SetKwInOut{Output}{Output}
    \Input{
        \textbf{T:} \dlifted IR. \\
        \textbf{T$^\prime$:} \compd lifted IR.
    }
    \Output{\textbf{True} $\implies$   \textbf{T} \& \textbf{T$^\prime$} 
        semantically equivalent \\
        \textbf{False} $\implies$   \textbf{T} \& \textbf{T$^\prime$} 
        \emph{may-be}
        non-equivalent
    }
    
    
    \BlankLine
    
    \ForEach{\textup{corresponding function pair (\F,\FP) in 
            (\T, \TP)}}{%
        
        \If{\textup{!Matcher(\F, \FP)}}{%
            \tcp{\textup{A potential bug in McSema while lifting \s{F}}}
            \KwRet{false}  \\ 
        }
        
    }
    \KwRet{true}
    %\NoCaptionOfAlgo
    \caption{\textbf{Matcher Strategy}}\label{alg:NM}
\end{algorithm}

The matcher algorithm is based on the following key observations on input IR 
programs, \s{T} \& \Tp, informally stated:
%
(I) Both exhibit identical control-flow and identical sequences of memory allocation
and reference behaviors (because McSema does not modify control flow or memory operations
during lifting).
%
(II) The single-instruction validation step proves that a memory store 
(respectively, load) in \s{T} writes (resp., reads) the equivalent set of memory
locations as does the corresponding operation in \Tp.  This property holds for
each dynamic instance of the corresponding instructions.

%
%, and 
%(III) There is no alloca instruction in either \T or \TP. All the load 
%(store) 
%instructions are reading from 
%(writing to) the Mcsema \Mcstate fields. This makes sense because for each 
%input \ISA instruction, both \T \& \TP simulates its read or write behavior on 
%register/flags/memory which are all modeled as fields in the \Mcstate 
%structure.

%Two instructions (one from each input program) which should match, as per its 
%data-flow behavior, may not occur in  their respective matching basic blocks. 
%This 
%is because  As the two input program 
%are not exactly equal to begin with, the instruction can get reordered 
%differently,

These two observations motivate an intuitively simple graph isomorphism strategy for
proving the equivalence of \T and \Tp.
%
Let us name the normalized versions of the function pair, \F \& \FP, as \FN \& 
\FNP.
The Matcher algorithm works on data dependence graphs, \GN \& \GNP, generated  
from \FN \& \FNP. A vertex of the graph represents an 
LLVM instruction and an edge between two vertices captures SSA def-use  or   
memory 
dependence relations. Memory dependence edges, extracted from alias analysis 
results, appear between LLVM load and 
store instructions.
\cmt{There is a particular reason why we do not add 
control-flow edges: It is evident from \emph{\textbf{Ob II}} that the input 
programs \T \& \TP, being not exactly equal to begin with, can be normalized 
differently and there is no guarantee that two matching instructions will end 
up in matching basic blocks. }
%\cmt{We 
%call an 
%instruction in \s{N} \emph{matching exactly} with 
%an instruction in \s{N$^\prime$}  if the containing sub-graphs are 
%isomorphic.} 

\paragraph{Soundness of Equivalence via Graph Isomorphism}
%
We argue informally that isomorphism of \GN \& \GNP implies semantic equivalence of
the programs \s{T} and \Tp. We consider all of memory used in an execution as a single
``SSA variable,'' which gets renamed at every store operation in the program.  A store
modifies some (unknown) subset of the locations in memory, and a load reads some
(unknown) subset of the bytes.  Given two isomorphic graphs \GN and \GNP, consider a 
matching pair of nodes representing a store instruction \s{S} in \s{T} and the 
corresponding store \Sp in \Tp.  A key to the correctness argument, below, is that
single-instruction validation proves that, if the initial state of memory and registers
is identical before executing \s{S} and \Sp, then the final state of memory and registers
is also identical, i.e., the same bytes have been written into the corresponding memory
locations.  Similarly, a matching pair of load instructions transfers identical bytes
from memory to SSA registers in \s{T} and \Tp.
%
The correctness argument then works as follows:
%
\begin{enumerate}
  %
  \item A node \s{N} and corresponding node \Np are equivalent because of the 
  equivalence proof constructed by single-instruction validation (Section~\ref{sec:siv}).
  %
  \item An SSA edge \s{A} $\rightarrow$ \s{B} and corresponding edge 
  \Ap $\rightarrow$ \Bp carry identical bytes of data, because \s{A} is equivalent
  to \Ap and \s{B} is equivalent to \Bp, by (1), above.
  %
  \item A memory edge \s{S} $\rightarrow$ \s{L} representing a \emph{true} memory
  dependence (i.e., a store-to-load dependence) and corresponding edge \Sp $\rightarrow$
  \Lp carry identical bytes of data, because \s{S} and \Sp store identical bytes into
  identical memory locations, and \s{L} and \Lp read identical bytes from identical
  memory locations.  The argument for \emph{anti} and \emph{output} memory dependences
  is similar.
  %
\end{enumerate}

Note that the above argument is independent of the precision of any static analysis
used to identify memory dependences.  A highly imprecise analysis (e.g., one that
says every store-load or store-store pair may be aliased) might lead to a failure to
prove isomorphism between \s{T} and \Tp, but will not claim isomorphism if the two
programs are not equivalent.  In practice, we find in our experiments, described in 
Section~\ref{sec:eval}, that the memory dependence edges from such a highly imprecise
analysis do indeed reduce the success rate of the Matcher, but only by a small amount.
A more precise analysis may improve the success rate, reducing the number of false
negatives.

\paragraph{Checking Graph Isomorphism}
%
We build on a subgraph-isomorphism algorithm from Saltz 
et al.~\cite{Saltz2014}, named 
\emph{dual-simulation} (refer Algorithm~\ref{alg:DS}),  to check if both \GN \& \GNP are 
subgraph-isomorphic to each other. The algorithm, in general, first 
retrieves initial potential match sets, $\Phi$,  for each vertex in one 
graph based on semantic and/or neighborhood information in the other graph.
In our case, the initial potential match set for an instruction 
\IN in \GN contains all the instructions in \GNP which have the same 
instruction opcode. Also, if \IN has constant operands then its potential 
matches must share 
those.  
Then the algorithm iteratively prunes out elements from the 
potential match 
set of each vertex based on its parents/child relations until it reaches a 
fixed-point.
%
Therefore, nodes \s{A} and \Ap in \GN and \GNP will be marked as isomorphic if
they have identical (isomorphic) sets of predecessors and successors.
%
Two edges will be marked as isomorphic if their source and sink nodes are isomorphic.

\begin{algorithm}
    \SetKwInOut{Input}{Inputs}
    \SetKwInOut{Output}{Output}
    \Input{ \\
        \textbf{\GN:} data-dependence graph of \N. \\
        \textbf{\GNP:} data-dependence graph of  \NP.
    }
    \Output{Check if \textbf{\GN} \& \textbf{\GNP} are isomorphic}
    \BlankLine

    changed $\gets$ true \\
    \While{changed} { 
       changed $\gets$ false \\
       \For{\un $\gets$ \GN } {
           \For{ \up $\gets$ \GN.adj(\un)} {
               \potpup $\gets$ $\emptyset$ \\
               \For{ \vn $\gets$ \potu } {
                   \potvup $\gets$ \GN.adj(\vn) $\cap$ \potup \\
                   \If{\potvup = $\empty$} {
                       remove \vn from \potu \\
                       \If{\potu = $\emptyset$} {
                           \KwRet{$\emptyset$} \\
                       }
                       changed $\gets$ true \\
                   }
                   \potpup $\gets$ \potpup $\cap$ \potvup \\
               }
               \If{ \potpup = $\emptyset$} {
                   \KwRet{$\emptyset$} \\
               }
               \If{ \potpup is smaller than \potup} {
                   changed $\gets$ true \\
               }
               \potup $\gets$ \potup $\cap$ \potpup \\
           }
       }
   }
   \KwRet{$\emptyset$} \\
    %\NoCaptionOfAlgo
    \caption{\textbf{Dual Simulation}}\label{alg:DS}
\end{algorithm}

%\cmt{Then we augmented the algorithm to infer the basic-block 
%correspondence 
%on the fly and use that information to prune out the potential sets of store 
%instructions.}

%\paragraph{\textup{\GN} \& \textup{\GNP} are non-isomorphic} 
%This could happen because the input functions (\F \& \FP), being not exactly 
%equal to begin with (from Obs. II), undergo different 
%optimizations. 
%
%There is one significant difference in the code in \s{T} and \Tp.
%%
%In \T, the addresses computations, using 
%\emph{getelelementptr} (gep in short) instructions, of all the simulated 
%registers 
%and flags  
%are hoisted in the \emph{entry} basic block. \mcsema does this as an 
%optimization so that  the subsequent data-dependent  
%instructions does not have to compute them again. 
%Whereas, in \TP  addresses of relevant registers and/or flags are recomputed at 
%every instruction site.  The normalizer is quite effective at transforming the two
%code versions to be similar, so that the data dependence graphs are isomorphic.
%
%
%As a simple example, consider the code snippets of the normalized function pair 
%(\FN \& \FNP). \FN is generated by normalizing \F, wherein the  
%simulated register \& sub-register address computations  are hoisted in 
%\reg{entry} block and 
%re-used at use-site. Whereas, \FNP is generated from \FP where the 
%simulated address is re-computed 
%at every use-site. \F and \FP upon normalization undergo 
%different optimizations; One with better CSE (common subexpr. elim.) than 
%the other.   
%\begin{lstlisting}[style=LLVMWOBORDER]
%          (*\FN*)                            (*\FNP*)
%%entry:                           %entry:
%  %expr = gep ...                 %expr = gep 
%  %cl =  gep %expr ...            %cl =  gep %expr 
%  %rcx = gep %expr ...            %rcx =  gep ...   
%  ...                               ... 
%%somebb:                          %somebb:
%  store to %rcx                      store to %rcx
%  ...                                ...
%%otherbb:                         %otherbb:
%  store to %cl                       store to %cl  
%  ...                                ... 
%\end{lstlisting}
%Clearly, the  \FN \& \FNP are equivalent, yet the naive isomorphism based 
%matcher has to declare them as non-equivalent because the corresponding graphs 
%are
%not isomorphic with the node \reg{cmn\_expr} in \GN has two out-edges versus 1 
%edge in \GNP. However, the key insight is: the  subgraph with nodes 
%\reg{expr}, 
%\reg{cl} and \s{store \%cl} in \GNP shares no data-dependent edges with the 
%rest of the graph and is isomorphic with the corresponding subgraph in \GN. We 
%can prune both the subgraph from their respective parent graphs and the 
%residual graphs, upon re-normalization, have better opportunity to get 
%normalized to isomorphic graphs.
%
%Again, consider the code snippets below. 
%%As before, \F has the   
%%simulated address computation, hoisted  in \reg{entry}, re-used in all its 
%%use-site.  
%%Whereas, \FNP is generated from \FP where the simulated address is re-computed 
%%at every use-site. 
%\FP upon normalization undergo 
%partial-redundancy-elimination to make the computation of \s{rax} 
%\emph{available} in both the paths (\reg{b0} and \reg{b1}), but missed 
% the opportunity to eliminate the common-subexpression, despite of the fact 
% that \s{pre\_rax} has no data-dependence on ``some-code''.      
%
%\begin{lstlisting}[style=LLVMWOBORDER]
%         (*\FN*)                            (*\FNP*)
%%entry:                         %entry:
% %rax =  gep ...                 ...   
% ...
% br %some_cond, %b0, %b1         br %some_cond, %b0, %b1
%%b0:                           %b0:
%    .. some-code ..               .. some-code ..
%                                 pre_rax = gep ...
%    br %merge                    br %merge
%%b1:                           %b1: 
%    store to %rax                rax1 = gep ... 
%                                 store to %rax1   
%    ; ...                        ; ...
%    br %merge                    br %merge
%%merge:                        %merge: 
%    store to %rax                rax2 = (*$\phi$*) [rax1, %b1], 
%                                         [%pre_rax, %b0 ]
%                                 store to %rax2
%\end{lstlisting}
%As before, despite the equivalence of \FN \& \FNP, the naive 
%matcher will fail to proof graph-isomorphism, resulting in false alarm.
%However, if we find the sub-graphs corresponding to ``some-code'' on either 
%side are matching exactly, then we can follow  the pruning strategy as before, 
%followed by normalization, and converge to isomorphic graphs.
%
%With that insight, we design the following iterative matching and pruning 
%algorithm (Algorithm~\ref{alg:Match}).  
%\begin{algorithm}
%    \SetKwInOut{Input}{Inputs}
%    \SetKwInOut{Output}{Output}
%    \Input{ \\
%        \textbf{\F:} \dlifted function. \\
%        \textbf{\FP:} \compd lifted function.
%    }
%    \Output{Check if \textbf{\F} \& \textbf{\FP} are semantically 
%        equivalent}
%    \BlankLine
%        itr $\gets$ MaxIter \\
%        \While{\textup{itr != 0}} {
%            $\FN \gets \textup{llvm-opt -O3 } (\F)$ \\
%            $\FNP \gets \textup{llvm-opt -O3 } (\FP)$ \\
%            
%            \GN $\gets$  data-dependence graph of \FN \\
%            \GNP $\gets$ data-dependence graph of \FNP \\
%            
%            \If{\GN \& \GNP are isoporphic} {
%                \KwRet{true}
%            }
%            
%            $(\F, \FP) \gets \textup{Prune isoporphic subgraphs from \GN \& 
%            \GNP}$
%            \BlankLine
%            itr $\gets$ itr - 1 \\
%        }
%        \KwRet{false}         
%    %\NoCaptionOfAlgo
%    \caption{\textbf{Matcher}}\label{alg:Match}
%\end{algorithm}
%The Matcher algorithm, being agnostic of the normalization pass, tries to 
%recover missed optimization opportunities during normalization. This ensures 
%that there will be very less false alarms.

%\todo[inline]{The soundness problem that Vikram pointed out: Removing code
%    might introduce undefined behavior, which the later opt passes might abuse
%    and we might end up getting false positive or false negatives.}


 

\paragraph{Comparison with LLVM-MD \& Peggy}
At this point, it is important to differentiate our approach to establish 
equivalence between two \LLVM programs, using  normalization followed by 
matching, 
from some of the existing
approaches for validating LLVM IR-to-IR optimization
passes, e.g. LLVM-MD~\cite{Tristan:2011} and Peggy~\cite{Stepp:2011}, which, 
like our approach, move away from simulation proofs, and instead use graph 
isomorphism techniques to prove equivalence. 
Both build graphs of expressions for each program, 
transform the graphs via a series of ``expert-provided'' rewrite rules, and 
check for equality. The rewrite-rules mimic various compiler-IR optimizations 
and hence the technique is precise when the output program is an 
optimization of the input program and the optimizations are captured by the 
rewrite  rules. 

Compared to these approaches, our normalizer is simpler, requires no additional 
implementation effort, and re-uses off-the-shelf, well-tested compiler passes 
to reduce the two programs to syntactic equivalence. Nevertheless, the 
normalizer is still very effective as shown in our evaluations.

%In our case, \s{T} (the \dlifted program) and \s{T$^\prime$} (the \compd 
%lifted 
%program) are structurally separated by idioms which 
%might be beyond the capability of compiler optimization passes to match 
%%%syntactically.
%Encoding such idioms as rewrite rules would make the 
%approach tied to a specific lifter, which is something we avoided by 
%making the matcher iterative. \todo[inline]{too heavy-weight for our purpose}

%\paragraph{Semantics Preserving Transformation}




%Matching expressions with complex Ï†-nodes seems well within
%the reach of any SMTprover. Our preliminary experiments with Z3 suggest that
%it can easily handle the sort of equivalences we need to show. However, this
%seems like a very heavy-weight tool. One question in our minds is whether or
%not there is an effective tech- nique somewhere in the middle: more
%sophisticated than syntactic matching, but short of a full SMT prover.
%\begin{lstlisting}[style=LLVMWOBORDER]
%          (*\F*)                            (*\FN*)
%%entry:                                 %entry:
% ; chain of geps to step                  %comon_expr = gep ...    
% ; thr. nested State struct               %cl =  gep %common_expr  
% ; to compute address of cl               %rcx = gep %common_expr
% %cl =  gep ...                           ...
%
% ; chain of geps to step
% ; thr. nested State struct
% ; to compute address of rcx 
% %rcx =  gep ...  
% ...
%%somebb:                                %somebb:
% store to %rcx                            store to %rcx                        
% ...                                      ...  
%%otherbb:                               %otherbb:   
% store to %cl                             store to %cl
% ...                                      ... 
%\end{lstlisting}
%Next,  looks at the code snippets of \FP and its normalized version \FNP.
%\begin{lstlisting}[style=LLVMWOBORDER]
%          (*\FP*)                            (*\FNP*)
%%entry:                                 %entry:
%                                          %common_expr = gep ...
%                                          %cl =  gep ... 
%                                          %rcx =  gep ... 
% ...                                      ...
%%somebb:                                %somebb:
% ; chain of geps            
% %cl =  gep ...
% store to %rcx                            store to %rcx                        
% ...                                      ...  
%%otherbb:                               %otherbb:   
% ; chain of geps
% %rcx =  gep ...  
% store to %cl                             store to %cl
% ...                                      ... 
%\end{lstlisting}
